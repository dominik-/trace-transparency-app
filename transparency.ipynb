{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import matplotlib.pyplot as plt\n",
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_date='2020-02-17T164323'\n",
    "#folder_results='results'\n",
    "#file_prefix='results-'\n",
    "file_name='traces.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file=folder_results+'/'+file_prefix+experiment_date+'/'+file_name\n",
    "cassandra_traces = pd.read_csv(file_name)\n",
    "tracedata = pd.DataFrame(cassandra_traces)\n",
    "#tracedata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify root traces\n",
    "1. For each root trace, find children\n",
    "1. Merge transparency tags as \"Sets\"\n",
    "1. Map tags to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#tracedata.describe()\n",
    "printable = tracedata.sort_values(by = ['trace_id']).reset_index(drop=True)\n",
    "#print(printable.dtypes)\n",
    "#conv = printable.astype({'span_id': 'object'})\n",
    "printable['span_id'] = printable['span_id'].apply(str)\n",
    "print(printable.head(9).to_latex(buf = 'tex-table.txt', columns = ['trace_id', 'span_id', 'duration', 'operation_name', 'tags'], formatters = {'trace_id': lambda x: x[-14:], 'span_id': lambda x: x[-5:]}))\n",
    "relevant = tracedata.filter(items=['trace_id','span_id','operation_name', 'refs', 'tags'])\n",
    "relevant['is_root'] = relevant['refs'].isna()\n",
    "relevant = relevant.sort_values(by = ['trace_id', 'is_root']).reset_index(drop=True)\n",
    "#relevant.head()\n",
    "#print(relevant.head(7).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting from cassandra with JSON column\n",
    "As it turns out, cassandra has its own internal model for JSON-formatted columns, which on export to CSV, are not properly formatted JSON (missing double quotes for keys). For SELECT queries in CQL, there is a JSON modifier, that fixes the output for columns, but there is none for CQL's COPY.\n",
    "\n",
    "- [x] TODO: find a way to parse the column of the CSV or export manually by running SELECT and writing results..?\n",
    "    - Solution: use regexp parsing\n",
    "- [x] TODO: re-generate output and CSV, so purposes are separated by something else than comma, so we can regexp-parse the malformed JSON output easier.\n",
    "    - used semicolon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i, row in relevant.iterrows():\n",
    "    #preprocessing...\n",
    "    relevant.at[i, 'refs'] = '{\"refs\": '+re.sub(\"(\\w+):\\s'?([\\w*\\-.;:\\w*]*)'?\", r'\"\\1\": \"\\2\"', str(row.refs))+'}'\n",
    "    relevant.at[i, 'tags'] = '{\"tags\": '+re.sub(\"(\\w+):\\s'?([\\w*\\-.;:\\w*]*)'?\", r'\"\\1\": \"\\2\"', str(row.tags))+'}'\n",
    "#relevant['tags'].head(5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "## traverse dict a and merge dict b\n",
    "def load_dict(val):\n",
    "    #TODO: later remove artificially added key in preprocessing, so json.loads gets us a list of objects directly\n",
    "    dict_raw = json.loads(val)\n",
    "    #flatten dict\n",
    "    listOfObjects = dict_raw['tags']\n",
    "    new = {}\n",
    "    for obj in listOfObjects:\n",
    "        if len(obj['value_string'].split(';')) > 1:\n",
    "            new[obj['key']] = []\n",
    "            for item in obj['value_string'].split(';'):\n",
    "                new[obj['key']].append(item)\n",
    "        else:\n",
    "            new[obj['key']] = obj['value_string']\n",
    "    return new\n",
    "\n",
    "#based on: https://stackoverflow.com/questions/7204805/how-to-merge-dictionaries-of-dictionaries/7205107#7205107\n",
    "def deep_merge(a, b, raw=True, path=None):\n",
    "    \"merges b into a\"\n",
    "    if raw:\n",
    "        b = load_dict(b)\n",
    "    if path is None: path = []\n",
    "    for key in b:\n",
    "        if key in a:\n",
    "            if type(a[key]) is not list:\n",
    "                a[key] = [a[key]]\n",
    "            if type(b[key]) is list:\n",
    "                for val in b[key]:\n",
    "                    a[key].append(val)\n",
    "            else:\n",
    "                a[key].append(b[key])\n",
    "        else:\n",
    "            a[key] = b[key]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i, row in relevant.head(10).iterrows():\n",
    " #   objects = load_dict(row.tags)\n",
    " #   print(objects)\n",
    "\n",
    "def split_colons_to_dict(items):\n",
    "    all_items_as_dict = {}\n",
    "    for item in items:\n",
    "        keyval = item.split(':')\n",
    "        tmp_dict = {keyval[0]: keyval[1]}\n",
    "        deep_merge(all_items_as_dict, tmp_dict, raw=False)\n",
    "    return all_items_as_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all tag data for each trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_id_old = ''\n",
    "tagset = {}\n",
    "tagsets = {}\n",
    "for i, row in relevant.iterrows():\n",
    "    if row.trace_id == trace_id_old:\n",
    "        tagset = deep_merge(tagset, row.tags)\n",
    "    else:\n",
    "        #remove duplicates from list\n",
    "        for key, val in tagset.items():\n",
    "            if type(val) is list:\n",
    "                tagset[key] = list(set(val))\n",
    "        tagsets[trace_id_old] = tagset\n",
    "        tagset = load_dict(row.tags)\n",
    "        trace_id_old = row.trace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is a colon in the list of values, split these and merge them into a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tagset in tagsets.values():\n",
    "    for key, values in tagset.items():\n",
    "        if type(values) is list:\n",
    "            if ':' in values[0]:\n",
    "                tagset[key] = split_colons_to_dict(values)\n",
    "#tagsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply 'max' filtering to storage-ttl values (because the policy should reflect worst-case storage durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tagset in tagsets.values():\n",
    "    for key, values in tagset.items():\n",
    "        if key == \"storage-ttl\":\n",
    "            for cat, ttls in values.items():\n",
    "                tagset[\"storage-ttl\"][cat] = max(ttls).zfill(2)\n",
    "#tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('vocab.json', 'r') as vocab_file:\n",
    "    vocab = json.load(vocab_file)\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Traces, Lookup Values in the Vocab\n",
    "* iterate over traces\n",
    "    * iterate over transparency keys\n",
    "        * for each key, lookup mapping in translation\n",
    "        * for each key, lookup key in vocab\n",
    "        * in category of vocab, lookup \"id\" for each item in list of values, add \"name\" field to policy.\n",
    "        * if we get an error (e.g., a missing key), this hints at an issue in our policy description\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_translation = {\n",
    "    \"recipients\": \"third-parties\",\n",
    "    \"sources\": \"third-parties\",\n",
    "    \"legal-basis\": \"legal-bases\",\n",
    "    \"data-categories\": \"categories\",\n",
    "    \"storage-ttl\": {\"categories\": \"ttl\"},\n",
    "    \"legitimate-interest\": \"legitimate-interests\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_policies = {}\n",
    "for traceid in tagsets.keys():\n",
    "    tmp_policy = {}\n",
    "    for cat_key, cat_value in tagsets[traceid].items():\n",
    "        category = cat_key\n",
    "        dict_mapping = False\n",
    "        if cat_key in lookup_translation:\n",
    "            category = lookup_translation[cat_key]\n",
    "        if type(category) is dict:\n",
    "            #if the looked up mapping is a dict, we assume policy items to be in dict format, too\n",
    "            for key, val in category.items():\n",
    "                tmp_policy[cat_key] = []\n",
    "                for id_key, id_val in cat_value.items():\n",
    "                    if id_val != '' and id_key != '':\n",
    "                        tmp_policy[cat_key].append({vocab[key][id_key]['name']: vocab[val][id_val]['name']})\n",
    "        if type(cat_value) is list:\n",
    "            tmp_policy[cat_key] = []\n",
    "            if category in vocab.keys():\n",
    "                for id_val in cat_value:\n",
    "                    if id_val != \"\":\n",
    "                        tmp_policy[cat_key].append(vocab[category][id_val]['name'])\n",
    "        if type(cat_value) is str:\n",
    "            if (dict_mapping == False and category in vocab.keys()):\n",
    "                tmp_policy[cat_key] = vocab[category][cat_value]['name']\n",
    "    #print(tmp_policy)\n",
    "    request_policies[traceid] = tmp_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request_policies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
